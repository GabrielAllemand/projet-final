import language_tool_python
from fastapi import APIRouter
from pydantic import BaseModel

# Assurez-vous que l'instance de LanguageTool est cr√©√©e
try:
    tool = language_tool_python.LanguageTool('fr')
except Exception as e:
    print(f"Erreur lors de l'initialisation de LanguageTool: {e}")
    tool = None # G√©rer le cas o√π l'outil ne peut pas √™tre initialis√©


router = APIRouter()

EXERCISES = {
    "Grammaire": [
        "Corrige la phrase : 'Il ont mang√© les pommes.'",
        "Transforme cette phrase au pass√© compos√©."
    ],
    "Vocabulaire": [
        "Trouve le synonyme de 'rapide'.",
        "Donne une phrase avec le mot '√©loquent'."
    ],
    "Oral": [
        "R√©p√®te ce texte : Les chaussettes de l'archiduchesse sont-elles s√®ches, archi-s√®ches ?",
        "R√©p√®te ce virelangue : Un chasseur sachant chasser doit savoir chasser sans son chien."
    ]
}

class ExerciseEvaluationRequest(BaseModel):
    user: str
    category: str
    question: str
    answer: str # R√©ponse texte de l'utilisateur (peut aussi servir pour la transcription brute initiale)
    transcription_data: dict | None = None # Ajouter ce champ pour les donn√©es de transcription orale

class EvaluationResult(BaseModel):
    score: int
    message: str
    corrections: list = [] # Liste des corrections LanguageTool
    transcription_data: dict | None = None # Pour les r√©sultats de transcription si applicable


@router.get("/list")
def get_exercises():
    return EXERCISES

@router.post("/evaluate", response_model=EvaluationResult)
async def evaluate_exercise(request: ExerciseEvaluationRequest):
    # Trouver l'exercice correspondant (v√©rification basique)
    if request.category not in EXERCISES or request.question not in EXERCISES[request.category]:
        return EvaluationResult(score=0, message="Exercice ou cat√©gorie invalide.")

    # Logique d'√©valuation
    if request.category != "Oral":
        # √âvaluation pour les exercices texte (Grammaire, Vocabulaire)
        if tool:
            matches = tool.check(request.answer)
            corrected_text = language_tool_python.utils.correct(request.answer, matches)
            
            corrections_list = []
            score = 100 # Score initial parfait
            if matches:
                score = max(0, 100 - (len(matches) * 10)) # Exemple simple de scoring bas√© sur le nombre d'erreurs
                for match in matches:
                     corrections_list.append({
                         "context": match.context,
                         "message": match.message,
                         "replacements": match.replacements
                     })

            message = f"Votre r√©ponse : {request.answer}\nCorrection sugg√©r√©e : {corrected_text}"
            return EvaluationResult(score=score, message=message, corrections=corrections_list)
        else:
             return EvaluationResult(score=0, message="Outil de correction linguistique non disponible.")
             
    else:
        # √âvaluation pour les exercices oraux am√©lior√©e
        if request.transcription_data:
            transcription = request.transcription_data.get("transcription", "").strip()
            corrected_transcription = request.transcription_data.get("corrected_transcription", "").strip()
            word_count = request.transcription_data.get("word_count", 0)
            speech_rate = request.transcription_data.get("speech_rate", 0)
            tic_counts = request.transcription_data.get("tic_counts", {})
            
            expected_text = request.question.replace("R√©p√®te ce texte : ", "").strip()

            # Logique d'√©valuation orale am√©lior√©e
            score = 100 # Score initial
            feedback_message = f"Exercice : {request.question}\n\nVotre transcription brute : {transcription}\nTranscription corrig√©e : {corrected_transcription}\n"

            # Comparaison de la transcription corrig√©e avec le texte attendu (casse-insensible pour la comparaison simple)
            if corrected_transcription.lower() != expected_text.lower():
                 # P√©nalit√© pour diff√©rence de contenu
                 score = max(0, score - 40) # Exemple: -40 points si le contenu n'est pas exactement correct
                 feedback_message += f"\nüí° Conseil : La transcription ne correspond pas exactement au texte attendu."
                 # Id√©alement, ici on calculerait une similarit√© pour un score partiel

            # P√©nalit√© pour les tics de langage
            total_tics = sum(tic_counts.values())
            if total_tics > 0:
                 tic_penalty = total_tics * 5 # Exemple: -5 points par tic
                 score = max(0, score - tic_penalty)
                 feedback_message += f"\n\nü§î Tics d√©tect√©s : {', '.join([f'{tic} ({count})' for tic, count in tic_counts.items()])}. Essayez de les r√©duire !"
            else:
                 feedback_message += "\n\nüéâ Aucun tic d√©tect√© !"
                 
            # Feedback sur la vitesse de parole (sans impact sur le score pour l'instant)
            if word_count > 0 and speech_rate > 0:
                 feedback_message += f"\n\n‚è± Vitesse de parole : {speech_rate:.1f} mots/min."
                 # Vous pourriez ajouter ici une logique pour commenter la vitesse (trop rapide/lente) si vous avez des seuils cibles.

            # S'assurer que le score ne descend pas en dessous de z√©ro
            score = max(0, score)

            return EvaluationResult(
                score=score,
                message=feedback_message,
                transcription_data=request.transcription_data # Renvoyer les donn√©es de transcription compl√®tes
            )
        else:
             return EvaluationResult(score=0, message="Donn√©es de transcription manquantes pour l'√©valuation orale.")

@router.get("/")
async def read_exercises():
    return {"message": "Exercises router is working"} 